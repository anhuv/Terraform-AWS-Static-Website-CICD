[
{
	"uri": "https://anhuv.github.io/cicd_lambda_canary_deployment/1-iac/1.1-asg/",
	"title": "AWS Auto Scaling Groups",
	"tags": [],
	"description": "",
	"content": "AWS Auto Scaling Groups AWS Auto Scaling Groups (ASGs) let you quickly scale and manage a collection of EC2 instances that run the same instance configuration. ASGs automatically scale the number of instances in response to changes in demand or other scaling policies. They ensure that the desired number of instances are always running, helping to maintain application availability and handle fluctuating workloads.\nScaling policies define the conditions under which the group scales up or down, such as CPU utilization, network traffic, or other custom metrics.\nTo utilize an auto-scaling group, you need to have a clear understanding of your application’s scaling requirements to be able to define appropriate policies. The best way to achieve that understanding is a period of monitoring the performance under load testing so you can accurately determine the metrics you’ll need to use in the scaling policy.\n"
},
{
	"uri": "https://anhuv.github.io/cicd_lambda_canary_deployment/2-cicd/2.1-codedeploy/",
	"title": "Creating a GitHub Repository and CodeDeploy Application",
	"tags": [],
	"description": "",
	"content": "Creating a GitHub Repository and CodeDeploy Application To trigger deployments, create a GitHub repository containing your code.\nOnce you have the repository, proceed with the following steps:\nGo to the AWS CodeDeploy service.\nCreate an application and select “EC2/on-premises” as the compute platform. Create a deployment group under this application.\nFor the deployment group, create a service role with the following steps:\nGo to “Roles” and select the use case as “CodeDeploy.” -After creating the service role, copy its ARN. -Return to the deployment group configuration and paste the ARN in the service role entry. -Select the autoscaling group and target group created earlier. -Set the deployment settings to “codedeploydefault.oneAtATime.” "
},
{
	"uri": "https://anhuv.github.io/cicd_lambda_canary_deployment/",
	"title": "Implement static website on Autoscaling Group with Terraform and AWS codepipeline",
	"tags": [],
	"description": "",
	"content": "Implement static website on Autoscaling Group with Terraform and AWS codepipeline Overview A CI/CD Pipeline with GitHub, Terraform, AWS CodePipeline, CodeDeploy, Load Balancer, Auto Scaling Group, and Target Group is an essential automated system in modern software development processes. Utilizing GitHub for source code management, Terraform for infrastructure provisioning and deployment, AWS CodePipeline and CodeDeploy for automating deployment processes, along with Load Balancer, Auto Scaling Group, and Target Group for enhancing system stability and scalability.\nContent Infrastructure as Code with Terraform CI/CD Pipeline with GitHub, AWS Code Pipeline, Code Deploy "
},
{
	"uri": "https://anhuv.github.io/cicd_lambda_canary_deployment/1-iac/",
	"title": "Infrastructure as Code with Terraform",
	"tags": [],
	"description": "",
	"content": "IaC with Terraform Terraform is an open-source infrastructure as code software tool created by HashiCorp. It allows users to define and provision data center infrastructure using a high-level configuration language known as HashiCorp Configuration Language (HCL), or optionally JSON. Terraform enables developers to define their infrastructure requirements in a declarative manner, specifying the desired state of their infrastructure rather than writing procedural scripts.\nOne of the key features of Terraform is its ability to manage a wide variety of cloud service providers, including Amazon Web Services (AWS), Microsoft Azure, Google Cloud Platform (GCP), and many others. This flexibility allows organizations to adopt a multi-cloud strategy or migrate between cloud providers with minimal friction.\nTerraform operates by creating an execution plan based on the desired infrastructure configuration and then applying that plan to provision the necessary resources. It also maintains a state file that records the current state of the infrastructure, allowing Terraform to track changes and manage updates effectively.\nWith Terraform, infrastructure can be managed and version-controlled alongside application code, promoting infrastructure as code practices and enabling collaboration between development and operations teams. This approach improves consistency, repeatability, and scalability while reducing the risk of configuration drift and manual errors.\nOverall, Terraform simplifies the process of managing infrastructure by automating provisioning, ensuring consistency across environments, and facilitating infrastructure changes through version-controlled configuration files.\nContents AWS Auto Scaling Groups Deploying an AWS Autoscaling Group with Terraform Result In the following sections, we will delve into the fundamental concepts of VPC.\n"
},
{
	"uri": "https://anhuv.github.io/cicd_lambda_canary_deployment/2-cicd/",
	"title": "CI/CD Pipeline with GitHub, AWS Code Pipeline, Code Deploy",
	"tags": [],
	"description": "",
	"content": "CI/CD Pipeline with GitHub, AWS Code Pipeline, Code Deploy Continuous Integration and Continuous Deployment (CI/CD) practices have become essential for modern software development. In this article, we will explore how to implement CI/CD using AWS CodePipeline and CodeDeploy. We’ll walk through the entire process, from creating launch templates to setting up an auto-scaling group, configuring CodeDeploy, and finally creating a CodePipeline for seamless deployment.\nContents Creating a GitHub Repository and CodeDeploy Application Setting up CodePipeline Testing the CI/CD Pipeline "
},
{
	"uri": "https://anhuv.github.io/cicd_lambda_canary_deployment/1-iac/1.2-deploy/",
	"title": "Deploying an AWS Autoscaling Group with Terraform",
	"tags": [],
	"description": "",
	"content": "Step 1: Clone source code from repository Open AWS CloudShell or AWS CLI Clone the Repository: git clone git@github.com:anhuv/codepipeline_ec2_auto_scaling.git cd iac/ Step 2: Review https://aws.plainenglish.io/deploying-a-aws-autoscaling-group-with-terraform-f487b865444f\nhttps://medium.com/@kelvi030117/cicd-for-high-availability-using-aws-codepipeline-aws-autoscaling-group-aws-load-balancer-7e6b26410a2e\nStart, view our code in the main.tf First we will start with our VPC\nresource \u0026#34;aws_vpc\u0026#34; \u0026#34;vpc\u0026#34; { cidr_block = var.vpc_cidr tags = { Name = local.vpc_name } } resource \u0026#34;aws_internet_gateway\u0026#34; \u0026#34;internet_gateway\u0026#34; { vpc_id = aws_vpc.vpc.id tags = { Name = local.internet_gateway_name } } resource \u0026#34;aws_subnet\u0026#34; \u0026#34;public_subnet\u0026#34; { count = 2 vpc_id = aws_vpc.vpc.id cidr_block = var.public_subnet_cidr[count.index] map_public_ip_on_launch = true availability_zone = var.az_names[count.index] tags = { Name = join(\u0026#34;-\u0026#34;, [local.public_subnet_name, var.az_names[count.index]]) } } resource \u0026#34;aws_subnet\u0026#34; \u0026#34;private_subnet\u0026#34; { count = 2 vpc_id = aws_vpc.vpc.id cidr_block = var.private_subnet_cidr[count.index] availability_zone = var.az_names[count.index] tags = { Name = join(\u0026#34;-\u0026#34;, [local.private_subnet_name, var.az_names[count.index]]) } } resource \u0026#34;aws_route_table\u0026#34; \u0026#34;public_route_table\u0026#34; { vpc_id = aws_vpc.vpc.id route { cidr_block = \u0026#34;0.0.0.0/0\u0026#34; gateway_id = aws_internet_gateway.internet_gateway.id } tags = { Name = local.public_route_table_name } } resource \u0026#34;aws_eip\u0026#34; \u0026#34;elastic_ip\u0026#34; { tags = { Name = local.elastic_ip_name } } resource \u0026#34;aws_nat_gateway\u0026#34; \u0026#34;nat_gateway\u0026#34; { allocation_id = aws_eip.elastic_ip.id connectivity_type = \u0026#34;public\u0026#34; subnet_id = aws_subnet.public_subnet[0].id tags = { Name = local.nat_gateway_name } depends_on = [aws_internet_gateway.internet_gateway] } resource \u0026#34;aws_route_table\u0026#34; \u0026#34;private_route_table\u0026#34; { vpc_id = aws_vpc.vpc.id route { cidr_block = \u0026#34;0.0.0.0/0\u0026#34; nat_gateway_id = aws_nat_gateway.nat_gateway.id } tags = { Name = local.private_route_table_name } } resource \u0026#34;aws_route_table_association\u0026#34; \u0026#34;public_rt_assoc\u0026#34; { count = 2 subnet_id = aws_subnet.public_subnet[count.index].id route_table_id = aws_route_table.public_route_table.id } resource \u0026#34;aws_route_table_association\u0026#34; \u0026#34;private_rt_assoc\u0026#34; { count = 2 subnet_id = aws_subnet.private_subnet[count.index].id route_table_id = aws_route_table.private_route_table.id } In the permission policy, select “AmazonEC2RoleforAWSCodeDeploy” and create the role using Terraform\nresource \u0026#34;aws_iam_role\u0026#34; \u0026#34;codedeploy\u0026#34; { name = local.iam_role_codedeploy_name assume_role_policy = jsonencode({ Version = \u0026#34;2012-10-17\u0026#34; Statement = [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;ec2.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] }) } resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;codedeploy\u0026#34; { policy_arn = \u0026#34;arn:aws:iam::aws:policy/service-role/AmazonEC2RoleforAWSCodeDeploy\u0026#34; role = aws_iam_role.codedeploy.name } then we will view our Security Group Resources\n# Security Group Resources resource \u0026#34;aws_security_group\u0026#34; \u0026#34;alb_security_group\u0026#34; { name = local.alb_security_group_name description = \u0026#34;ALB Security Group\u0026#34; vpc_id = aws_vpc.vpc.id ingress { description = \u0026#34;HTTP from Internet\u0026#34; from_port = 80 to_port = 80 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } tags = { Name = local.alb_security_group_name } } resource \u0026#34;aws_security_group\u0026#34; \u0026#34;asg_security_group\u0026#34; { name = local.asg_security_group_name description = \u0026#34;ASG Security Group\u0026#34; vpc_id = aws_vpc.vpc.id ingress { description = \u0026#34;HTTP from ALB\u0026#34; from_port = 80 to_port = 80 protocol = \u0026#34;tcp\u0026#34; security_groups = [aws_security_group.alb_security_group.id] } egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } tags = { Name = local.asg_security_group_name } } next lets create the launch template with previously created role and the auto scaling group resources\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;example\u0026#34; { ami = \u0026#34;ami-0c55b159cbfafe1f0\u0026#34; instance_type = \u0026#34;t2.micro\u0026#34; } next lets create the launch template with previously created role and the auto scaling group resources\nresource \u0026#34;aws_launch_template\u0026#34; \u0026#34;launch_template\u0026#34; { name = local.launch_template_name image_id = var.ami instance_type = var.instance_type network_interfaces { device_index = 0 security_groups = [aws_security_group.asg_security_group.id] } iam_instance_profile { name = local.iam_role_codedeploy_name } tag_specifications { resource_type = \u0026#34;instance\u0026#34; tags = { Name = local.launch_template_ec2_name } } user_data = filebase64(\u0026#34;${path.module}/input-bash.sh\u0026#34;) } resource \u0026#34;aws_autoscaling_group\u0026#34; \u0026#34;auto_scaling_group\u0026#34; { desired_capacity = var.desired_capacity max_size = var.max_size min_size = var.min_size vpc_zone_identifier = [for i in aws_subnet.private_subnet[*] : i.id] target_group_arns = [aws_lb_target_group.target_group.arn] launch_template { id = aws_launch_template.launch_template.id version = aws_launch_template.launch_template.latest_version } } next the Application Load Balancer resources\n# Application Load Balancer Resources resource \u0026#34;aws_lb\u0026#34; \u0026#34;alb\u0026#34; { name = local.alb_name internal = false load_balancer_type = \u0026#34;application\u0026#34; security_groups = [aws_security_group.alb_security_group.id] subnets = [for i in aws_subnet.public_subnet : i.id] } resource \u0026#34;aws_lb_target_group\u0026#34; \u0026#34;target_group\u0026#34; { name = local.target_group_name port = 80 protocol = \u0026#34;HTTP\u0026#34; vpc_id = aws_vpc.vpc.id health_check { path = \u0026#34;/\u0026#34; matcher = 200 } } resource \u0026#34;aws_lb_listener\u0026#34; \u0026#34;alb_listener\u0026#34; { load_balancer_arn = aws_lb.alb.arn port = \u0026#34;80\u0026#34; protocol = \u0026#34;HTTP\u0026#34; default_action { type = \u0026#34;forward\u0026#34; target_group_arn = aws_lb_target_group.target_group.arn } } bash script to install Nginx\n#!/bin/bash sudo su apt update apt upgrade -y apt install nginx -y sudo apt update sudo apt install ruby-full -y sudo apt install wget cd /home/ubuntu wget https://aws-codedeploy-us-east-1.s3.us-east-1.amazonaws.com/latest/install chmod +x ./install sudo ./install auto Let view variables.tf\n# Terraform Variables # Local Values locals { vpc_name = \u0026#34;anhuv-vnet\u0026#34; internet_gateway_name = \u0026#34;anhuv-internet-gateway\u0026#34; public_subnet_name = \u0026#34;anhuv-public-subnet\u0026#34; private_subnet_name = \u0026#34;anhuv-private-subnet\u0026#34; public_route_table_name = \u0026#34;anhuv-public-route-table\u0026#34; private_route_table_name = \u0026#34;anhuv-private-route-table\u0026#34; elastic_ip_name = \u0026#34;anhuv-nat-elastic-ip\u0026#34; nat_gateway_name = \u0026#34;anhuv-nat-gateway\u0026#34; alb_security_group_name = \u0026#34;anhuv-alb-security-group\u0026#34; asg_security_group_name = \u0026#34;anhuv-asg-security-group\u0026#34; launch_template_name = \u0026#34;anhuv-launch-template\u0026#34; launch_template_ec2_name = \u0026#34;anhuv-asg-ec2\u0026#34; alb_name = \u0026#34;anhuv-external-alb\u0026#34; target_group_name = \u0026#34;anhuv-alb-target-group\u0026#34; iam_role_codedeploy_name = \u0026#34;anhuv-codedeploy-ec2-role\u0026#34; } # VPC Variables variable \u0026#34;vpc_cidr\u0026#34; { description = \u0026#34;VPC cidr block\u0026#34; type = string default = \u0026#34;10.1.0.0/16\u0026#34; } variable \u0026#34;az_names\u0026#34; { type = list(string) default = [\u0026#34;us-east-1a\u0026#34;, \u0026#34;us-east-1b\u0026#34;] } variable \u0026#34;public_subnet_cidr\u0026#34; { description = \u0026#34;Public Subnet cidr block\u0026#34; type = list(string) default = [\u0026#34;10.1.0.0/24\u0026#34;, \u0026#34;10.1.1.0/24\u0026#34;] } variable \u0026#34;private_subnet_cidr\u0026#34; { description = \u0026#34;Private Subnet cidr block\u0026#34; type = list(string) default = [\u0026#34;10.1.10.0/24\u0026#34;, \u0026#34;10.1.11.0/24\u0026#34;] } # Launch Template and ASG Variables variable \u0026#34;ami\u0026#34; { description = \u0026#34;ami id\u0026#34; type = string default = \u0026#34;ami-07d9b9ddc6cd8dd30\u0026#34; } variable \u0026#34;aws_region\u0026#34; { description = \u0026#34;AWS region name\u0026#34; type = string default = \u0026#34;us-east-1\u0026#34; } variable \u0026#34;server_port\u0026#34; { description = \u0026#34;The port the web server will be listening\u0026#34; type = number default = 8080 } variable \u0026#34;elb_port\u0026#34; { description = \u0026#34;The port the elb will be listening\u0026#34; type = number default = 80 } variable \u0026#34;instance_type\u0026#34; { description = \u0026#34;The type of EC2 Instances to run (e.g. t2.micro)\u0026#34; type = string default = \u0026#34;t2.micro\u0026#34; } variable \u0026#34;min_size\u0026#34; { description = \u0026#34;The minimum number of EC2 Instances in the ASG\u0026#34; type = number default = 1 } variable \u0026#34;max_size\u0026#34; { description = \u0026#34;The maximum number of EC2 Instances in the ASG\u0026#34; type = number default = 2 } variable \u0026#34;desired_capacity\u0026#34; { description = \u0026#34;The desired number of EC2 Instances in the ASG\u0026#34; type = number default = 1 } Step 3: Deploying using Terraform Initialize Terraform: Navigate to your Terraform project directory using the command line interface (CLI) and run terraform init. This command initializes the working directory, downloads any required plugins, and prepares Terraform for configuration. Plan Infrastructure Changes: Run terraform plan to create an execution plan. Terraform will analyze your configuration and generate an execution plan detailing what actions it will take to achieve the desired state of your infrastructure. Review Plan: Review the output of the terraform plan command to ensure that Terraform will make the desired changes. This step allows you to verify the proposed changes before applying them.\nApply Changes: Once you are satisfied with the plan, execute terraform apply to apply the changes to your infrastructure. Terraform will prompt for confirmation before proceeding with any modifications.\nVerify Changes: After Terraform applies the changes, verify that your infrastructure has been provisioned or updated according to the specified configuration.\n"
},
{
	"uri": "https://anhuv.github.io/cicd_lambda_canary_deployment/2-cicd/2.2-codepipeline/",
	"title": "Setting up CodePipeline",
	"tags": [],
	"description": "",
	"content": "Setting up CodePipeline Now, let’s configure CodePipeline to automate the deployment process. Follow these steps:\nGo to the AWS CodePipeline service. Create a new pipeline and select GitHub as the source provider(version 2), Connect your GitHub account and authorize access. Choose the repository containing your code and configure the relevant details. Skip the build stage for now. In the deploy stage, select “AWS CodeDeploy” as the deploy provider. Choose the application name and deployment group that we created earlier. Finally, create the pipeline. "
},
{
	"uri": "https://anhuv.github.io/cicd_lambda_canary_deployment/1-iac/1.3-output/",
	"title": "Results",
	"tags": [],
	"description": "",
	"content": "Results VPC Subnets EC2 Role “AmazonEC2RoleforAWSCodeDeploy” created by terraform Application Load Balancer Nginx Server "
},
{
	"uri": "https://anhuv.github.io/cicd_lambda_canary_deployment/2-cicd/2.3-testing/",
	"title": "Testing the CI/CD Pipeline",
	"tags": [],
	"description": "",
	"content": "Testing the CI/CD Pipeline Make a change to the code in the main branch of your GitHub repository.\nBefore: After: "
},
{
	"uri": "https://anhuv.github.io/cicd_lambda_canary_deployment/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://anhuv.github.io/cicd_lambda_canary_deployment/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]